
"""
AI Ä°stemci ModÃ¼lÃ¼ (LangChain Versiyonu)

Google Gemini API kullanarak persona tabanlÄ± sohbet uygulamasÄ± (LangChain ile yeniden yazÄ±ldÄ±).
"""


# Gerekli kÃ¼tÃ¼phaneler import ediliyor
import os
from typing import Optional, Dict, Any, List
import google.generativeai as genai  # genai sadece anahtar yapÄ±landÄ±rmasÄ± iÃ§in bÄ±rakÄ±ldÄ±
from dotenv import load_dotenv  # Ortam deÄŸiÅŸkenlerini yÃ¼klemek iÃ§in
from langchain_google_genai import ChatGoogleGenerativeAI  # Gemini model entegrasyonu
from langchain.schema import HumanMessage, SystemMessage, AIMessage, BaseMessage  # Mesaj tipleri
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate  # Prompt ÅŸablonlarÄ±
from langchain.schema.runnable import RunnableConfig  # Zincir yapÄ±landÄ±rmasÄ±


# Genel aÃ§Ä±klama:
# Bu modÃ¼l, ortam deÄŸiÅŸkenlerinden yapÄ±landÄ±rma okuyup (Ã¶r. GEMINI_API_KEY,
# BACKEND_PANEL_URL, ABLY bilgileri), persona verisini backend veya Ably'den
# Ã§ekip LangChain / Google Gemini (ChatGoogleGenerativeAI) modeli ile etkileÅŸen
# bir sohbet dÃ¶ngÃ¼sÃ¼ saÄŸlar.


# VarsayÄ±lan persona modÃ¼lÃ¼ import'u (orijinal koddan)
# GerÃ§ek bir uygulamada, fetch_persona'nÄ±n LangChain/API uyumlu olmasÄ± gerekebilir.
try:
    from persona import fetch_persona
except ImportError:
    print("âš ï¸ 'persona.py' modÃ¼lÃ¼ bulunamadÄ±. Persona yÃ¼klemesi atlanacaktÄ±r.")
    # Fallback fonksiyonu: persona.py yoksa tip uyumluluÄŸu iÃ§in aynÄ± imzaya
    # sahip bir stub tanÄ±mlÄ±yoruz. Tip olarak Optional[Dict[str, Any]] dÃ¶ner.
    def fetch_persona(
        api_url: Optional[str] = None,
        token: Optional[str] = None,
        timeout: int = 5,
        ably_channel: Optional[str] = None,
        ably_api_key: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:
        return None


# --- Sabitler ---
# Model sÄ±caklÄ±ÄŸÄ± (cevaplarÄ±n Ã§eÅŸitliliÄŸi)
DEFAULT_TEMPERATURE = 0.7
# Maksimum Ã§Ä±ktÄ± token sayÄ±sÄ± (kullanÄ±lmÄ±yor ama ileride eklenebilir)
MAX_OUTPUT_TOKENS = 500
# Gemini model adÄ±
GEMINI_MODEL_NAME = "gemini-2.5-flash"
# LangChain iÃ§in model versiyonu
CHAT_MODEL_ID = "gemini-2.5-flash"

# Sohbetten Ã§Ä±kÄ±ÅŸ komutlarÄ±
EXIT_COMMANDS = ["exit", "q", "quit"]



# AI istemci hatalarÄ±nÄ± temsil eden Ã¶zel exception
class AIClientError(Exception):
    """AI istemci hatalarÄ±nÄ± temsil eder."""
    pass



def load_api_configuration() -> str:
    """
    Ortam deÄŸiÅŸkenlerinden API anahtarÄ±nÄ± yÃ¼kler ve doÄŸrular.
    Gemini API anahtarÄ± eksik veya geÃ§ersizse hata fÄ±rlatÄ±r.
    """
    load_dotenv()  # .env dosyasÄ±nÄ± yÃ¼kler
    
    # Gemini iÃ§in anahtarÄ±n GEMINI_API_KEY olmasÄ± beklenir.
    api_key = os.getenv("GEMINI_API_KEY") 
    
    # Anahtar yoksa veya dummy ise hata fÄ±rlatÄ±lÄ±r
    if not api_key or "dummy" in api_key.lower():
        raise AIClientError(
            "GEMINI_API_KEY eksik veya geÃ§ersiz. "
            "LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin."
        )
    
    return api_key



def initialize_langchain_model(api_key: str) -> ChatGoogleGenerativeAI:
    """
    LangChain ChatGoogleGenerativeAI modelini yapÄ±landÄ±rÄ±r ve baÅŸlatÄ±r.
    Google Gemini API anahtarÄ± ile LangChain modelini baÅŸlatÄ±r.
    """
    # Ortam deÄŸiÅŸkeni ayarlanÄ±yor (LangChain otomatik Ã§ekebilsin diye)
    if "GEMINI_API_KEY" not in os.environ:
         os.environ["GEMINI_API_KEY"] = api_key
         
    # Model nesnesi oluÅŸturuluyor
    model = ChatGoogleGenerativeAI(
        model=CHAT_MODEL_ID,
        temperature=DEFAULT_TEMPERATURE,
        # streaming=True  # AkÄ±ÅŸ isteseydik buraya eklenebilirdi
    )
    print(f"âœ… LangChain ChatModel ({CHAT_MODEL_ID}) baÅŸarÄ±yla baÅŸlatÄ±ldÄ±.")
    return model


# --- Backend ve Persona Ä°ÅŸlevleri (Orijinal mantÄ±k korunmuÅŸtur) ---


def load_backend_configuration() -> Dict[str, Optional[str]]:
    """
    Ortam deÄŸiÅŸkenlerinden backend ve Ably yapÄ±landÄ±rmasÄ±nÄ± yÃ¼kler.
    Sohbet iÃ§in gerekli baÄŸlantÄ± bilgilerini dÃ¶ndÃ¼rÃ¼r.
    """
    return {
        "url": os.getenv("BACKEND_PANEL_URL"),
        "token": os.getenv("BACKEND_PANEL_TOKEN"),
        "ably_channel": os.getenv("BACKEND_PANEL_ABLY_CHANNEL"),
        "ably_api_key": os.getenv("BACKEND_PANEL_ABLY_API_KEY")
    }



def load_persona(config: Dict[str, Optional[str]]) -> Optional[Dict[str, Any]]:
    """
    Backend veya Ably'den persona verisini yÃ¼kler.
    Persona verisi yoksa veya hata olursa None dÃ¶ner.
    """
    if not (config["url"] or config["ably_channel"]):
        return None
    
    try:
        # Persona verisi Ã§ekiliyor
        persona = fetch_persona(
            config["url"],
            token=config["token"],
            ably_channel=config["ably_channel"],
            ably_api_key=config["ably_api_key"]
        )
        # persona None dÃ¶nebilir; bu nedenle Ã¶nce dict olduÄŸundan emin ol
        persona_name = persona.get("name") if isinstance(persona, dict) and persona.get("name") else "(isimsiz)"
        print(f"âœ… Persona backend'den yÃ¼klendi: {persona_name}")
        return persona

    except Exception as e:
        print(f"âš ï¸ Persona backend'den yÃ¼klenemedi: {e}")
        return None



def build_persona_prefix(persona: Dict[str, Any]) -> str:
    """
    Persona sÃ¶zlÃ¼ÄŸÃ¼nden Ã¶zet bir string oluÅŸturur.
    Ad, ton ve kÄ±sÄ±tlamalarÄ± birleÅŸtirir.
    """
    parts = []
    
    if persona.get("name"):
        parts.append(f"Persona adÄ±: {persona.get('name')}")
    
    if persona.get("tone"):
        parts.append(f"Ton: {persona.get('tone')}")
    
    if persona.get("constraints"):
        parts.append(f"KÄ±sÄ±tlamalar: {persona.get('constraints')}")
    
    # ParÃ§alarÄ± birleÅŸtirip dÃ¶ndÃ¼r
    return " | ".join(parts) if parts else ""



def create_persona_system_message(persona: Optional[Dict[str, Any]]) -> Optional[SystemMessage]:
    """
    Persona bilgilerinden bir SystemMessage oluÅŸturur.
    Persona Ã¶zetini sistem mesajÄ± olarak dÃ¶ndÃ¼rÃ¼r.
    """
    if not persona:
        return None
    
    persona_prefix = build_persona_prefix(persona)
    
    if persona_prefix:
        # SystemMessage, modelin davranÄ±ÅŸÄ±nÄ± yÃ¶nlendirmek iÃ§in en uygunudur.
        return SystemMessage(content=f"Sen daima ÅŸu kurallara uyacaksÄ±n: {persona_prefix}.")
    
    return None


# --- LangChain Entegrasyonu ve Sohbet DÃ¶ngÃ¼sÃ¼ ---


def create_prompt_chain(system_message: Optional[SystemMessage]) -> ChatPromptTemplate:
    """
    Sistem mesajÄ±nÄ± (persona) ve kullanÄ±cÄ± girdisini birleÅŸtiren zinciri oluÅŸturur.
    Sohbet iÃ§in prompt zinciri hazÄ±rlanÄ±r.
    """
    # Temel mesajlar listesi hazÄ±rlanÄ±yor
    # Not: LangChain tipleri ve kullanÄ±lan sÃ¼rÃ¼mler arasÄ±nda type-hint uyuÅŸmazlÄ±klarÄ±
    # olabiliyor (Ã¶r. PromptTemplate vs BaseMessage). Statik analizÃ¶rlerin fazla
    # agresif hata vermesini engellemek iÃ§in burada daha gevÅŸek bir tip kullanÄ±yoruz.
    messages: List[Any] = []
    
    # Persona varsa ekle
    if system_message:
        # BazÄ± LangChain sÃ¼rÃ¼mlerinde SystemMessage.content'in tipi
        # beklenmeyen bir yapÄ± (Ã¶r. liste veya dict) olabilir; buradan
        # gÃ¼venli bir string geÃ§irerek type-hata riskini azaltÄ±yoruz.
        template_text = system_message.content if isinstance(system_message.content, str) else str(system_message.content)
        messages.append(SystemMessagePromptTemplate.from_template(template_text))
        
    # KullanÄ±cÄ± prompt'u ekleniyor
    messages.append(HumanMessagePromptTemplate.from_template("{user_prompt}"))
    
    # ChatPromptTemplate oluÅŸturulup dÃ¶ndÃ¼rÃ¼lÃ¼yor
    return ChatPromptTemplate.from_messages(messages)



def run_chat_loop(
    model: ChatGoogleGenerativeAI,
    persona: Optional[Dict[str, Any]]
) -> None:
    """
    Ana sohbet dÃ¶ngÃ¼sÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±r (LangChain kullanÄ±larak).
    KullanÄ±cÄ±dan prompt alÄ±r, modele gÃ¶nderir ve yanÄ±tÄ± ekrana basar.
    """
    system_message = create_persona_system_message(persona)
    
    # Prompt Zincirini OluÅŸtur
    prompt_template = create_prompt_chain(system_message)
    
    # Zinciri oluÅŸtur: Prompt Template -> LLM
    # LangChain'in modern 'Runnable' yapÄ±sÄ± kullanÄ±lÄ±r.
    chain = prompt_template | model

    print("ğŸ’¬ Sohbeti sonlandÄ±rmak iÃ§in 'exit', 'q' veya 'quit' yazÄ±n.\n")
    
    if system_message:
        print(f"âš™ï¸ Persona Etkin: {system_message.content}")
        
    while True:
        try:
            # KullanÄ±cÄ±dan prompt alÄ±nÄ±yor
            user_prompt = input("ğŸ§  Prompt: ").strip()
              
            # Ã‡Ä±kÄ±ÅŸ kontrolÃ¼
            if user_prompt.lower() in EXIT_COMMANDS:
                print("ğŸ‘‹ Sohbet sonlandÄ±rÄ±ldÄ±.")
                break
              
            # BoÅŸ prompt kontrolÃ¼
            if not user_prompt:
                print("âš ï¸ LÃ¼tfen geÃ§erli bir prompt girin.")
                continue
              
            print("\nâ³ YanÄ±t oluÅŸturuluyor...\n")
            
            # Zinciri Ã‡alÄ±ÅŸtÄ±r
            # Zincire gÃ¶nderilecek input dictionary formatÄ±nda olmalÄ±dÄ±r.
            response_message = chain.invoke(
                {"user_prompt": user_prompt},
                config=RunnableConfig()
            )

            # YanÄ±t mesaj nesnesinden metni al
            # Model adaptÃ¶rleri farklÄ± tiplerde content dÃ¶nebilir (str, list, dict, vs.).
            raw_content = getattr(response_message, "content", None)

            response_text: Optional[str]
            if isinstance(raw_content, list):
                # Liste halinde parÃ§alar gelebilir; string olanlarÄ± birleÅŸtir
                parts: List[str] = []
                for item in raw_content:
                    if isinstance(item, str):
                        parts.append(item)
                    elif hasattr(item, "text"):
                        parts.append(str(getattr(item, "text")))
                    else:
                        parts.append(str(item))
                response_text = " ".join(parts).strip() if parts else None

            elif isinstance(raw_content, str):
                response_text = raw_content.strip() if raw_content else None

            else:
                # SÃ¶zlÃ¼k/nesne gibi diÄŸer tipler iÃ§in gÃ¼venli bir deneme
                try:
                    if isinstance(raw_content, dict) and raw_content.get("text"):
                        response_text = str(raw_content.get("text")).strip()
                    else:
                        response_text = str(raw_content).strip() if raw_content else None
                except Exception:
                    response_text = None

            # YanÄ±tÄ± gÃ¶ster
            display_response(response_text)
              
        except KeyboardInterrupt:
            print("\nğŸ›‘ KullanÄ±cÄ± tarafÄ±ndan durduruldu.")
            break
            
        except Exception as e:
            print(f"âŒ Beklenmeyen hata: {e}\n")



def display_response(response: Optional[str]) -> None:
    """
    Model yanÄ±tÄ±nÄ± formatlÄ± ÅŸekilde ekrana yazdÄ±rÄ±r.
    YanÄ±t yoksa uyarÄ± verir.
    """
    if not response:
        print("âš ï¸ Model boÅŸ bir yanÄ±t dÃ¶ndÃ¼rdÃ¼.")
        return
    
    print("ğŸ¤– YanÄ±t:")
    print("--------------------------------------------------")
    print(response)
    print("--------------------------------------------------\n")



def main() -> None:
    """
    Ana program giriÅŸ noktasÄ±.
    TÃ¼m adÄ±mlar sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r, hata olursa ekrana basÄ±lÄ±r.
    """
    try:
        # API anahtarÄ±nÄ± yÃ¼kle ve doÄŸrula
        api_key = load_api_configuration()
        
        # Gemini modelini LangChain ile baÅŸlat
        model = initialize_langchain_model(api_key)
        
        # Backend yapÄ±landÄ±rmasÄ±nÄ± yÃ¼kle
        backend_config = load_backend_configuration()
        
        # Persona'yÄ± yÃ¼kle (varsa)
        persona = load_persona(backend_config)
        
        # Sohbet dÃ¶ngÃ¼sÃ¼nÃ¼ baÅŸlat
        run_chat_loop(model, persona)
        
    except AIClientError as e:
        # API anahtarÄ± hatasÄ± ekrana basÄ±lÄ±r
        print(f"ğŸš¨ HATA: {e}")
        exit(1)
        
    except Exception as e:
        # DiÄŸer beklenmeyen hatalar ekrana basÄ±lÄ±r
        print(f"ğŸš¨ Beklenmeyen HATA: {e}")
        exit(1)



# Ana dosya olarak Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda ana fonksiyon baÅŸlatÄ±lÄ±r
if __name__ == "__main__":
    main()